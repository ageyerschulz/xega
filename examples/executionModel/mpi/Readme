
Scripts for using mpi with package Rmpi function Rmpi::mpi.parLapply() 
in user defined apply statement:

# The user defined parallel apply function
rmpiLapply<-function(pop, EvalGene, lF)
{
   Rmpi::mpi.parLapply(pop, FUN=EvalGene, lF=lF)
}

# The user defined parallel apply function
# which must be used with pipeline compilation

# The user defined parallel apply function
# Multiple R-slave processes may need a lot of memory:
# Trigger garbage collection with gc() in master and slaves more often.
rmpiLapply<-function(pop, EvalGene, lF)
{ gc()
  newpop<-lapply(pop, FUN=serialize, connection=NULL)
  newEvalGene<-function(gene, lF) {gc(); pipe<-unserialize(gene);pipe(lF)}
  Rmpi::mpi.parLapply(newpop, FUN=newEvalGene, lF=lF)
}

Use XORGPRmpiPipeline.R!

Scripts:

1. Notebook (Sequential):

   ./Rscript XORGPRmpi.R > output-benchNotebookSeq-23218642.log

   Output:
   output-benchNotebookSeq-23218642.log
   xegaResult2024-03-08-17-06-47.763237.rds

2. Notebook mpi with 8 cores:

   ./benchRmpiNotebook8.sh

   (uses Rscript XORGPRmpi.R)

   Output:
   - XORGPRmpi.out
   - xegaResult2024-03-01-10-40-45.002129.rds

   (Time stamp may vary.)

3. HPC Slurm with 20 nodes,  40 cores on each node (800 cores): 

   sbatch benchRmpiSlurm.sh

   (uses Rscript XORGPRmpi.R)

   Output:

   output-benchRmpiSlurm-23218642.log
   xegaResult2024-03-07-21-51-50.rds

4. Comparion of results:

   ComparisonNBHPC.lst



